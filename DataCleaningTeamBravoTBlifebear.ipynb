{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='data_cleaning.log',\n",
        "    filemode='w',  # Overwrite the log file each time\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Define the path to your CSV file\n",
        "csv_file_path = '/content/lifebear.csv'  # Update this path as needed\n",
        "\n",
        "# Define paths for cleaned data and garbage data\n",
        "cleaned_csv_path = '/content/lifebear_cleaned.csv'  # Update as needed\n",
        "garbage_csv_path = '/content/lifebear_garbage.csv'  # Update as needed\n",
        "\n",
        "# Function to count total rows (excluding header)\n",
        "def count_total_rows(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            total = sum(1 for line in f) - 1  # Subtract 1 for header\n",
        "        return total\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error counting rows: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to clean each chunk\n",
        "def clean_chunk(df):\n",
        "    try:\n",
        "        original_count = len(df)\n",
        "\n",
        "        # 1. Handle Missing Data\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "        # Fill numeric columns with mean\n",
        "        df_numeric_filled = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "        # Fill categorical columns with 'Unknown'\n",
        "        df_categorical_filled = df[categorical_cols].fillna('Unknown')\n",
        "\n",
        "        # Combine filled data\n",
        "        df = pd.concat([df_numeric_filled, df_categorical_filled], axis=1)\n",
        "\n",
        "        # 2. Remove Duplicates\n",
        "        duplicates_before = df.duplicated().sum()\n",
        "        df = df.drop_duplicates()\n",
        "        duplicates_after = df.duplicated().sum()\n",
        "        duplicates_removed = duplicates_before - duplicates_after\n",
        "        if duplicates_removed > 0:\n",
        "            logging.info(f\"Removed {duplicates_removed} duplicate rows\")\n",
        "\n",
        "        # 3. Convert Data Types\n",
        "        if 'age' in df.columns:\n",
        "            df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
        "        if 'date_column' in df.columns:\n",
        "            df['date_column'] = pd.to_datetime(df['date_column'], errors='coerce')\n",
        "\n",
        "        # After conversion, handle new missing values\n",
        "        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "        df[categorical_cols] = df[categorical_cols].fillna('Unknown')\n",
        "\n",
        "        # 4. Handle Outliers using IQR for a specific column, e.g., 'salary'\n",
        "        garbage_chunk = pd.DataFrame()\n",
        "        if 'salary' in df.columns:\n",
        "            Q1 = df['salary'].quantile(0.25)\n",
        "            Q3 = df['salary'].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            outliers = df[(df['salary'] < lower_bound) | (df['salary'] > upper_bound)]\n",
        "            num_outliers = len(outliers)\n",
        "            df = df[(df['salary'] >= lower_bound) & (df['salary'] <= upper_bound)]\n",
        "            if num_outliers > 0:\n",
        "                logging.info(f\"Removed {num_outliers} outlier rows based on 'salary'\")\n",
        "                garbage_chunk = pd.concat([garbage_chunk, outliers], axis=0)\n",
        "        else:\n",
        "            logging.warning(\"'salary' column not found for outlier removal\")\n",
        "\n",
        "        # 5. Rename Columns for Consistency\n",
        "        if 'Name' in df.columns:\n",
        "            df = df.rename(columns={'Name': 'name'})\n",
        "            logging.info(\"Renamed column 'Name' to 'name'\")\n",
        "\n",
        "        # 6. Standardize Data\n",
        "        if 'name' in df.columns:\n",
        "            df['name'] = df['name'].str.lower().str.strip()\n",
        "\n",
        "        # 7. Deal with Invalid Data\n",
        "        if 'age' in df.columns:\n",
        "            before_invalid = df[~df['age'].between(0, 120)]\n",
        "            num_invalid_age = len(before_invalid)\n",
        "            if num_invalid_age > 0:\n",
        "                logging.info(f\"Removed {num_invalid_age} rows with invalid 'age'\")\n",
        "                df = df[df['age'].between(0, 120)]\n",
        "                garbage_chunk = pd.concat([garbage_chunk, before_invalid], axis=0)\n",
        "\n",
        "        cleaned_count = len(df)\n",
        "        logging.info(f\"Cleaned chunk: {original_count} original rows, {cleaned_count} cleaned rows\")\n",
        "\n",
        "        return df, garbage_chunk  # Return cleaned data and garbage as separate DataFrames\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error cleaning chunk: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to process the CSV in specified number of chunks\n",
        "def process_csv_in_chunks(file_path, num_chunks, delimiter=';'):\n",
        "    try:\n",
        "        total_rows = count_total_rows(file_path)\n",
        "        chunk_size = total_rows // num_chunks\n",
        "        logging.info(f\"Processing CSV in {num_chunks} chunks of {chunk_size} rows each\")\n",
        "        print(f\"Processing CSV in {num_chunks} chunks of {chunk_size} rows each\")\n",
        "\n",
        "        cleaned_chunks = []\n",
        "\n",
        "        # Initialize garbage CSV: write header\n",
        "        with open(garbage_csv_path, 'w', encoding='utf-8') as f_garbage:\n",
        "            # Read the first chunk to get headers\n",
        "            first_chunk = pd.read_csv(file_path, delimiter=delimiter, nrows=chunk_size)\n",
        "            f_garbage.write(';'.join(first_chunk.columns) + '\\n')\n",
        "            del first_chunk  # Free memory\n",
        "\n",
        "        # Initialize the reader\n",
        "        reader = pd.read_csv(file_path, delimiter=delimiter, chunksize=chunk_size, low_memory=False)\n",
        "\n",
        "        for i, chunk in enumerate(reader, 1):\n",
        "            logging.info(f\"Processing chunk {i}/{num_chunks}\")\n",
        "            print(f\"Processing chunk {i}/{num_chunks}\")\n",
        "            try:\n",
        "                cleaned_chunk, garbage_chunk = clean_chunk(chunk)\n",
        "                cleaned_chunks.append(cleaned_chunk)\n",
        "\n",
        "                # Append garbage rows to garbage CSV\n",
        "                if not garbage_chunk.empty:\n",
        "                    garbage_chunk.to_csv(\n",
        "                        garbage_csv_path,\n",
        "                        mode='a',\n",
        "                        header=False,\n",
        "                        index=False,\n",
        "                        sep=';'\n",
        "                    )\n",
        "                    logging.info(f\"Appended {len(garbage_chunk)} garbage rows from chunk {i}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process chunk {i}: {e}\")\n",
        "\n",
        "        # Handle any remaining rows if total_rows is not divisible by num_chunks\n",
        "        remainder = total_rows % num_chunks\n",
        "        if remainder != 0:\n",
        "            logging.info(f\"Processing remaining {remainder} records\")\n",
        "            print(f\"Processing remaining {remainder} records\")\n",
        "            try:\n",
        "                last_chunk = pd.read_csv(\n",
        "                    file_path,\n",
        "                    delimiter=delimiter,\n",
        "                    skiprows=range(1, num_chunks * chunk_size + 1),\n",
        "                    nrows=remainder,\n",
        "                    header=None,\n",
        "                    names=chunk.columns\n",
        "                )\n",
        "                cleaned_last_chunk, garbage_last_chunk = clean_chunk(last_chunk)\n",
        "                cleaned_chunks.append(cleaned_last_chunk)\n",
        "\n",
        "                # Append garbage rows to garbage CSV\n",
        "                if not garbage_last_chunk.empty:\n",
        "                    garbage_last_chunk.to_csv(\n",
        "                        garbage_csv_path,\n",
        "                        mode='a',\n",
        "                        header=False,\n",
        "                        index=False,\n",
        "                        sep=';'\n",
        "                    )\n",
        "                    logging.info(f\"Appended {len(garbage_last_chunk)} garbage rows from the remaining chunk\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process remaining chunk: {e}\")\n",
        "\n",
        "        # Remerge all cleaned chunks into a single DataFrame\n",
        "        df_cleaned = pd.concat(cleaned_chunks, ignore_index=True)\n",
        "        df_cleaned.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        logging.info(\"Completed processing all chunks\")\n",
        "        print(\"Completed processing all chunks\")\n",
        "\n",
        "        return df_cleaned\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing CSV in chunks: {e}\")\n",
        "        raise\n",
        "\n",
        "# Execute the processing\n",
        "df_cleaned = process_csv_in_chunks(csv_file_path, num_chunks)\n",
        "\n",
        "# Display the first few rows of the cleaned DataFrame\n",
        "logging.info(\"Displaying the first few rows of the cleaned DataFrame\")\n",
        "print(\"\\nCleaned DataFrame Head:\")\n",
        "print(df_cleaned.head())\n",
        "\n",
        "# Display summary statistics\n",
        "logging.info(\"Displaying summary statistics of the cleaned DataFrame\")\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(df_cleaned.describe())\n",
        "\n",
        "# Display missing values after cleaning\n",
        "logging.info(\"Displaying missing values after cleaning\")\n",
        "print(\"\\nMissing Values After Cleaning:\")\n",
        "print(df_cleaned.isnull().sum())\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "try:\n",
        "    df_cleaned.to_csv(cleaned_csv_path, index=False, sep=';')\n",
        "    logging.info(f\"Cleaned data saved to {cleaned_csv_path}\")\n",
        "    print(f\"\\nCleaned data saved to {cleaned_csv_path}\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving cleaned data: {e}\")\n",
        "    print(f\"\\nError saving cleaned data: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "6Wt1Ryx_Sy33",
        "outputId": "62f81c09-6d25-48db-d8db-d53241bb2b0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_chunks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6d44e56af9c1>\u001b[0m in \u001b[0;36m<cell line: 194>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;31m# Execute the processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m \u001b[0mdf_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_csv_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;31m# Display the first few rows of the cleaned DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_chunks' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4u7rEoqPTB7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='data_cleaning.log',\n",
        "    filemode='w',  # Overwrite the log file each time\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Define the path to your CSV file\n",
        "csv_file_path = '/content/lifebear.csv'  # Update this path as needed\n",
        "\n",
        "# Define paths for cleaned data and garbage data\n",
        "cleaned_csv_path = '/content/lifebear_cleaned.csv'  # Update as needed\n",
        "garbage_csv_path = '/content/lifebear_garbage.csv'  # Update as needed\n",
        "\n",
        "# Function to count total rows (excluding header)\n",
        "def count_total_rows(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            total = sum(1 for line in f) - 1  # Subtract 1 for header\n",
        "        return total\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error counting rows: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to clean each chunk\n",
        "def clean_chunk(df):\n",
        "    # ... (This function remains unchanged) ...\n",
        "\n",
        "# Function to process the CSV in specified number of chunks\n",
        "def process_csv_in_chunks(file_path, num_chunks, delimiter=';'):\n",
        "    # ... (This function remains unchanged) ...\n",
        "\n",
        "# Execute the processing\n",
        "# Define the number of chunks here\n",
        "num_chunks = 5  # For example, divide the CSV into 5 chunks\n",
        "df_cleaned = process_csv_in_chunks(csv_file_path, num_chunks)\n",
        "\n",
        "# Display the first few rows of the cleaned DataFrame\n",
        "logging.info(\"Displaying the first few rows of the cleaned DataFrame\")\n",
        "print(\"\\nCleaned DataFrame Head:\")\n",
        "print(df_cleaned.head())\n",
        "\n",
        "# Display summary statistics\n",
        "logging.info(\"Displaying summary statistics of the cleaned DataFrame\")\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(df_cleaned.describe())\n",
        "\n",
        "# Display missing values after cleaning\n",
        "logging.info(\"Displaying missing values after cleaning\")\n",
        "print(\"\\nMissing Values After Cleaning:\")\n",
        "print(df_cleaned.isnull().sum())\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "try:\n",
        "    df_cleaned.to_csv(cleaned_csv_path, index=False, sep=';')\n",
        "    logging.info(f\"Cleaned data saved to {cleaned_csv_path}\")\n",
        "    print(f\"\\nCleaned data saved to {cleaned_csv_path}\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving cleaned data: {e}\")\n",
        "    print(f\"\\nError saving cleaned data: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "C_IJ0xsRTcqr",
        "outputId": "ade31a34-9c8c-4a4e-c6f8-1260ca751d76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 32 (<ipython-input-5-b96bd3c879ef>, line 36)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-b96bd3c879ef>\"\u001b[0;36m, line \u001b[0;32m36\u001b[0m\n\u001b[0;31m    def process_csv_in_chunks(file_path, num_chunks, delimiter=';'):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='data_cleaning.log',\n",
        "    filemode='w',  # Overwrite the log file each time\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Define the path to your CSV file\n",
        "csv_file_path = '/content/lifebear.csv'  # Update this path as needed\n",
        "\n",
        "# Define paths for cleaned data and garbage data\n",
        "cleaned_csv_path = '/content/lifebear_cleaned.csv'  # Update as needed\n",
        "garbage_csv_path = '/content/lifebear_garbage.csv'  # Update as needed\n",
        "\n",
        "# Function to count total rows (excluding header)\n",
        "def count_total_rows(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            total = sum(1 for line in f) - 1  # Subtract 1 for header\n",
        "        return total\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error counting rows: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to clean each chunk\n",
        "def clean_chunk(df):\n",
        "    try:\n",
        "        original_count = len(df)\n",
        "\n",
        "        # 1. Handle Missing Data\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "        # Fill numeric columns with mean\n",
        "        df_numeric_filled = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "        # Fill categorical columns with 'Unknown'\n",
        "        df_categorical_filled = df[categorical_cols].fillna('Unknown')\n",
        "\n",
        "        # Combine filled data\n",
        "        df = pd.concat([df_numeric_filled, df_categorical_filled], axis=1)\n",
        "\n",
        "        # 2. Remove Duplicates\n",
        "        duplicates_before = df.duplicated().sum()\n",
        "        df = df.drop_duplicates()\n",
        "        duplicates_after = df.duplicated().sum()\n",
        "        duplicates_removed = duplicates_before - duplicates_after\n",
        "        if duplicates_removed > 0:\n",
        "            logging.info(f\"Removed {duplicates_removed} duplicate rows\")\n",
        "\n",
        "        # 3. Convert Data Types\n",
        "        if 'age' in df.columns:\n",
        "            df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
        "        if 'date_column' in df.columns:\n",
        "            df['date_column'] = pd.to_datetime(df['date_column'], errors='coerce')\n",
        "\n",
        "        # After conversion, handle new missing values\n",
        "        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "        df[categorical_cols] = df[categorical_cols].fillna('Unknown')\n",
        "\n",
        "        # 4. Handle Outliers using IQR for a specific column, e.g., 'salary'\n",
        "        garbage_chunk = pd.DataFrame()\n",
        "        if 'salary' in df.columns:\n",
        "            Q1 = df['salary'].quantile(0.25)\n",
        "            Q3 = df['salary'].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            outliers = df[(df['salary'] < lower_bound) | (df['salary'] > upper_bound)]\n",
        "            num_outliers = len(outliers)\n",
        "            df = df[(df['salary'] >= lower_bound) & (df['salary'] <= upper_bound)]\n",
        "            if num_outliers > 0:\n",
        "                logging.info(f\"Removed {num_outliers} outlier rows based on 'salary'\")\n",
        "                garbage_chunk = pd.concat([garbage_chunk, outliers], axis=0)\n",
        "        else:\n",
        "            logging.warning(\"'salary' column not found for outlier removal\")\n",
        "\n",
        "        # 5. Rename Columns for Consistency\n",
        "        if 'Name' in df.columns:\n",
        "            df = df.rename(columns={'Name': 'name'})\n",
        "            logging.info(\"Renamed column 'Name' to 'name'\")\n",
        "\n",
        "        # 6. Standardize Data\n",
        "        if 'name' in df.columns:\n",
        "            df['name'] = df['name'].str.lower().str.strip()\n",
        "\n",
        "        # 7. Deal with Invalid Data\n",
        "        if 'age' in df.columns:\n",
        "            before_invalid = df[~df['age'].between(0, 120)]\n",
        "            num_invalid_age = len(before_invalid)\n",
        "            if num_invalid_age > 0:\n",
        "                logging.info(f\"Removed {num_invalid_age} rows with invalid 'age'\")\n",
        "                df = df[df['age'].between(0, 120)]\n",
        "                garbage_chunk = pd.concat([garbage_chunk, before_invalid], axis=0)\n",
        "\n",
        "        cleaned_count = len(df)\n",
        "        logging.info(f\"Cleaned chunk: {original_count} original rows, {cleaned_count} cleaned rows\")\n",
        "\n",
        "        return df, garbage_chunk  # Return cleaned data and garbage as separate DataFrames\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error cleaning chunk: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to process the CSV in specified number of chunks\n",
        "def process_csv_in_chunks(file_path, num_chunks, delimiter=';'):\n",
        "    try:\n",
        "        total_rows = count_total_rows(file_path)\n",
        "        chunk_size = total_rows // num_chunks\n",
        "        logging.info(f\"Processing CSV in {num_chunks} chunks of {chunk_size} rows each\")\n",
        "        print(f\"Processing CSV in {num_chunks} chunks of {chunk_size} rows each\")\n",
        "\n",
        "        cleaned_chunks = []\n",
        "\n",
        "        # Initialize garbage CSV: write header\n",
        "        with open(garbage_csv_path, 'w', encoding='utf-8') as f_garbage:\n",
        "            # Read the first chunk to get headers\n",
        "            first_chunk = pd.read_csv(file_path, delimiter=delimiter, nrows=chunk_size)\n",
        "            f_garbage.write(';'.join(first_chunk.columns) + '\\n')\n",
        "            del first_chunk  # Free memory\n",
        "\n",
        "        # Initialize the reader\n",
        "        reader = pd.read_csv(file_path, delimiter=delimiter, chunksize=chunk_size, low_memory=False)\n",
        "\n",
        "        for i, chunk in enumerate(reader, 1):\n",
        "            logging.info(f\"Processing chunk {i}/{num_chunks}\")\n",
        "            print(f\"Processing chunk {i}/{num_chunks}\")\n",
        "            try:\n",
        "                cleaned_chunk, garbage_chunk = clean_chunk(chunk)\n",
        "                cleaned_chunks.append(cleaned_chunk)\n",
        "\n",
        "                # Append garbage rows to garbage CSV\n",
        "                if not garbage_chunk.empty:\n",
        "                    garbage_chunk.to_csv(\n",
        "                        garbage_csv_path,\n",
        "                        mode='a',\n",
        "                        header=False,\n",
        "                        index=False,\n",
        "                        sep=';'\n",
        "                    )\n",
        "                    logging.info(f\"Appended {len(garbage_chunk)} garbage rows from chunk {i}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process chunk {i}: {e}\")\n",
        "\n",
        "        # Handle any remaining rows if total_rows is not divisible by num_chunks\n",
        "        remainder = total_rows % num_chunks\n",
        "        if remainder != 0:\n",
        "            logging.info(f\"Processing remaining {remainder} records\")\n",
        "            print(f\"Processing remaining {remainder} records\")\n",
        "            try:\n",
        "                last_chunk = pd.read_csv(\n",
        "                    file_path,\n",
        "                    delimiter=delimiter,\n",
        "                    skiprows=range(1, num_chunks * chunk_size + 1),\n",
        "                    nrows=remainder,\n",
        "                    header=None,\n",
        "                    names=chunk.columns\n",
        "                )\n",
        "                cleaned_last_chunk, garbage_last_chunk = clean_chunk(last_chunk)\n",
        "                cleaned_chunks.append(cleaned_last_chunk)\n",
        "\n",
        "                # Append garbage rows to garbage CSV\n",
        "                if not garbage_last_chunk.empty:\n",
        "                    garbage_last_chunk.to_csv(\n",
        "                        garbage_csv_path,\n",
        "                        mode='a',\n",
        "                        header=False,\n",
        "                        index=False,\n",
        "                        sep=';'\n",
        "                    )\n",
        "                    logging.info(f\"Appended {len(garbage_last_chunk)} garbage rows from the remaining chunk\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process remaining chunk: {e}\")\n",
        "\n",
        "        # Remerge all cleaned chunks into a single DataFrame\n",
        "        df_cleaned = pd.concat(cleaned_chunks, ignore_index=True)\n",
        "        df_cleaned.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        logging.info(\"Completed processing all chunks\")\n",
        "        print(\"Completed processing all chunks\")\n",
        "\n",
        "        return df_cleaned\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing CSV in chunks: {e}\")\n",
        "        raise\n",
        "\n",
        "# Execute the processing\n",
        "df_cleaned = process_csv_in_chunks(csv_file_path, 10)\n",
        "\n",
        "# Display the first few rows of the cleaned DataFrame\n",
        "logging.info(\"Displaying the first few rows of the cleaned DataFrame\")\n",
        "print(\"\\nCleaned DataFrame Head:\")\n",
        "print(df_cleaned.head())\n",
        "\n",
        "# Display summary statistics\n",
        "logging.info(\"Displaying summary statistics of the cleaned DataFrame\")\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(df_cleaned.describe())\n",
        "\n",
        "# Display missing values after cleaning\n",
        "logging.info(\"Displaying missing values after cleaning\")\n",
        "print(\"\\nMissing Values After Cleaning:\")\n",
        "print(df_cleaned.isnull().sum())\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "try:\n",
        "    df_cleaned.to_csv(cleaned_csv_path, index=False, sep=';')\n",
        "    logging.info(f\"Cleaned data saved to {cleaned_csv_path}\")\n",
        "    print(f\"\\nCleaned data saved to {cleaned_csv_path}\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error saving cleaned data: {e}\")\n",
        "    print(f\"\\nError saving cleaned data: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkvIQM8wTt8-",
        "outputId": "d2e9ce4b-5017-4e05-b6c3-4c29d402e4c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing CSV in 10 chunks of 368044 rows each\n",
            "Processing chunk 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n",
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 11/10\n",
            "Processing remaining 2 records\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'salary' column not found for outlier removal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing all chunks\n",
            "\n",
            "Cleaned DataFrame Head:\n",
            "  id gender    login_id              mail_address  \\\n",
            "0  1    0.0    sugimoto   sugimoto@lifebear.co.jp   \n",
            "1  2    0.0         kou  nakanishi@lifebear.co.jp   \n",
            "2  3    0.0      yusuke     yuozawa1208@gmail.com   \n",
            "3  4    0.0  entyan1106        endo1106@gmail.com   \n",
            "4  5    0.0      kuriki          kuriki@wavy4.com   \n",
            "\n",
            "                           password           created_at          salt  \\\n",
            "0  f0bac04aa1b45cf443d722d6f71c0250  2012-01-13 22:54:05  yGwBKynnsctI   \n",
            "1  48207c322ee5bb156ffec9f08c960aaa  2012-01-14 12:48:31  aha6EuRYCDvU   \n",
            "2  048261a8024ce51d379eb53cc51aaf33  2012-01-17 15:33:22  PVS59dPWk9BH   \n",
            "3  cd77a9dac26260a104facda5665eb3ab  2012-01-17 15:37:02  vLZI6TVCJowN   \n",
            "4  a026597c294cc48cd20ae361f10cbab1  2012-01-17 18:52:32  swFznWWk79fg   \n",
            "\n",
            "  birthday_on  \n",
            "0  1984-11-09  \n",
            "1  1986-11-13  \n",
            "2  1984-12-08  \n",
            "3  1987-11-06  \n",
            "4  1986-10-21  \n",
            "\n",
            "Summary Statistics:\n",
            "             id     gender login_id           mail_address  \\\n",
            "count   3680444  3680444.0  3680444                3680444   \n",
            "unique  3680444       15.0  3671195                3670681   \n",
            "top           1        1.0  aya3988  akstektfw39@gmail.com   \n",
            "freq          1  1531495.0      200                    201   \n",
            "\n",
            "                                password           created_at          salt  \\\n",
            "count                            3680444              3680444       3680444   \n",
            "unique                           3680395              3631814       3680395   \n",
            "top     89f94b85c2bef7ba1b507dc47128e137  2018-11-19 07:44:05  nkrpK2RFi8pS   \n",
            "freq                                  42                    4            42   \n",
            "\n",
            "       birthday_on  \n",
            "count       736090  \n",
            "unique       19876  \n",
            "top        Unknown  \n",
            "freq        350548  \n",
            "\n",
            "Missing Values After Cleaning:\n",
            "id                    0\n",
            "gender                0\n",
            "login_id              0\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     2944354\n",
            "dtype: int64\n",
            "\n",
            "Cleaned data saved to /content/lifebear_cleaned.csv\n"
          ]
        }
      ]
    }
  ]
}